{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anki.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Myt3nXoISkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61edb207-555b-4de5-fe2f-47ae8661784c"
      },
      "source": [
        "import pandas as pd\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsUdjzn-JJgR"
      },
      "source": [
        "def palvavra_separadas(dados, len_palavras = 2):\r\n",
        "  '''\r\n",
        "  ******************************************************************************\r\n",
        "  palvavra_separadas: Separa as palavras com mais de\r\n",
        "  len_palavras caracteres, retirando pontuacoes.\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  Input:\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  dados: Datafame\r\n",
        "  len_palavras : tamanho minimo das palavras que seram considerado (>2)\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  Outout:\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  tokens: listas com as palavras separadas\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  ******************************************************************************\r\n",
        "  '''\r\n",
        "  tokens = []\r\n",
        "  for frase in dados[\"tratados\"]:\r\n",
        "    for token in nltk.tokenize.word_tokenize(frase):\r\n",
        "      if token.isalpha() and len(token) > len_palavras:\r\n",
        "        tokens.append(token.lower())\r\n",
        "\r\n",
        "  return tokens\r\n",
        "\r\n",
        "def gera_palavras_unicas(palavras):\r\n",
        "  '''\r\n",
        "  ******************************************************************************\r\n",
        "  gera_palavras_unicas: Gera conjunto com palavras unicas.\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  Input:\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  palavra: lista com as palavras  \r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  Outout:\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  tokens: listas com as palavras separadas\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  ******************************************************************************\r\n",
        "  '''\r\n",
        "  unicas = set()\r\n",
        "\r\n",
        "  for palavra in palavras:\r\n",
        "    unicas.add(palavra)\r\n",
        "\r\n",
        "  return unicas"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dh8xfGUL2E5"
      },
      "source": [
        "def substituir(dados):\r\n",
        "  '''\r\n",
        "  ******************************************************************************\r\n",
        "  Tratando a base de dados.\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  Input:\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  dados: Datafame original\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  Outout:\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  dados: Datafame com coluna tratado\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  ******************************************************************************\r\n",
        "  '''\r\n",
        "  dados[\"tratados\"] = dados[\"front\"]\r\n",
        "\r\n",
        "  retirar = re.compile(\"\\[anki:play:q:0\\]\")\r\n",
        "  linhas = []\r\n",
        "  for linha in dados[\"tratados\"]:    \r\n",
        "    linhas.append(re.sub(retirar, '', linha))\r\n",
        "  dados[\"tratados\"] = linhas\r\n",
        "\r\n",
        "  retirar = re.compile(\"\\[Grammar\\]\")\r\n",
        "  linhas = []\r\n",
        "  for linha in dados[\"tratados\"]:    \r\n",
        "    linhas.append(re.sub(retirar, '', linha))\r\n",
        "  dados[\"tratados\"] = linhas\r\n",
        "\r\n",
        "  retirar = re.compile(\"\\[Inglês Britânico\\]\")\r\n",
        "  linhas = []\r\n",
        "  for linha in dados[\"tratados\"]:    \r\n",
        "    linhas.append(re.sub(retirar, '', linha))\r\n",
        "  dados[\"tratados\"] = linhas"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ecR5yLaeriT"
      },
      "source": [
        "def make_date(file_name):\r\n",
        "  '''\r\n",
        "  ******************************************************************************\r\n",
        "  make_date: Le o nome do arquivo e retorna a data.\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  Input:\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  file_name: nome do arquivo\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  Outout:\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  Data no formato 00/00/0000\r\n",
        "  ------------------------------------------------------------------------------\r\n",
        "  ******************************************************************************\r\n",
        "  '''\r\n",
        "  return file_name[5:14].replace(\"_\",\"/\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpPq4_iyc2K5"
      },
      "source": [
        "files = \"anki_11_02_2021.txt\", \"anki_20_02_2021.txt\", \"anki_08_03_2021.txt\""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyCdpdAFMzqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "196cdb0d-207e-4d0f-d682-649553583bb3"
      },
      "source": [
        "for file_name in files:\r\n",
        "  dados = pd.read_csv(file_name, sep=\"\\t\", header=None, names = [\"front\", \"back\"])\r\n",
        "  substituir(dados)\r\n",
        "  palavras = palvavra_separadas(dados)\r\n",
        "  total_palavras = len(palavras)\r\n",
        "  palavras_unicas = gera_palavras_unicas(palavras)\r\n",
        "  total_de_palavras_unicas = len(palavras_unicas)\r\n",
        "  print(f\"data                     : {make_date(file_name)}\")\r\n",
        "  print(f\"Total de palavras        : {total_palavras}\")\r\n",
        "  print(f\"Total de palavras unicas : {total_de_palavras_unicas}\")\r\n",
        "  print()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data                     : 11/02/202\n",
            "Total de palavras        : 22722\n",
            "Total de palavras unicas : 4862\n",
            "\n",
            "data                     : 20/02/202\n",
            "Total de palavras        : 23838\n",
            "Total de palavras unicas : 4980\n",
            "\n",
            "data                     : 08/03/202\n",
            "Total de palavras        : 27007\n",
            "Total de palavras unicas : 5396\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQQwxjxxVHFA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nAxGhnsaGa5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9CKta8CXCum",
        "outputId": "2e2e6f71-ded7-4449-b1b4-a0c92f9e79f3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de palavras        : 23838\n",
            "Total de palavras unicas : 4980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4AmDFdFlL82"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}